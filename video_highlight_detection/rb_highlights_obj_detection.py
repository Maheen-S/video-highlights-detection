# -*- coding: utf-8 -*-
"""rb_highlights_obj_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G2XIba2QDYgS7DAkyWiKIE_vufifhtJH
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2
import os
import torch
from pathlib import Path

# Commented out IPython magic to ensure Python compatibility.
# %pip install ultralytics
import ultralytics
ultralytics.checks()

from ultralytics import YOLO
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)
# model



def get_video_files(main_dir, extensions=['.mp4', '.avi', '.mkv']):
    video_files = []
    for root, dirs, files in os.walk(main_dir):
        for file in files:
            if any(file.endswith(ext) for ext in extensions):
                video_files.append(os.path.join(root, file))
    return video_files

main_dir = '/kaggle/input/sample-cricket-video-clips'

video_files = get_video_files(main_dir)
single_video_path = video_files[0]
single_video_path

video_frames = {}

def extract_frames(video_path, output_dir):
    video_name = os.path.basename(video_path).split('.')[0]
    video_frames[video_name] = []

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    vidcap = cv2.VideoCapture(video_path)
    success, image = vidcap.read()
    count = 0

    while success:
        frame_path = os.path.join(output_dir, f"frame{count}.jpg")
        cv2.imwrite(frame_path, image)
        video_frames[video_name].append(frame_path)
        success, image = vidcap.read()
        count += 1

output_dir = os.path.join('/kaggle/working/', os.path.basename(single_video_path).split('.')[0])
extract_frames(single_video_path, output_dir)



"""# YOLO with Bounding boxes and labels"""

import cv2
import os
import torch

model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

def process_video(video_path, output_video_path, model, condition_func, fps=30):
    vidcap = cv2.VideoCapture(video_path)
    success, frame = vidcap.read()

    if not success:
        print("Failed to open video")
        return

    # video prop
    height, width, _ = frame.shape
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

    frame_count = 0
    while success:
        #  YOLO
        results = model(frame)
        boxes = results.xyxy[0]  # bB

        # Analyze if frame should be included
        if condition_func(frame, boxes):
            # Draw bounding boxes and labels on the frame
            for box in boxes:
                x1, y1, x2, y2, conf, cls = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4], int(box[5])
                label = model.names[int(cls)]
                confidence = box[4]

                # Draw the bounding box
                frame = cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

                # Prepare the label text
                label_text = f'{label} {confidence:.2f}'
                label_size, _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
                label_ymin = max(y1, label_size[1] + 10)

                # Draw the label background
                frame = cv2.rectangle(frame, (x1, label_ymin - label_size[1] - 10),
                                      (x1 + label_size[0], label_ymin), (0, 255, 0), cv2.FILLED)

                # Put the label text above the bounding box
                frame = cv2.putText(frame, label_text, (x1, label_ymin - 7),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)

            # Write frame to output video
            out.write(frame)

        # Read next frame
        success, frame = vidcap.read()
        frame_count += 1

    vidcap.release()
    out.release()
    print(f"Processed {frame_count} frames.")

def condition_func(frame, boxes):
    # Example condition: include frame if any box has confidence > 0.8
    return any(box[4] > 0.8 for box in boxes)

# Parameters

video_path = "/kaggle/input/sample-cricket-video-clips/Sample Cricket Video Clips/Sample Cricket Video Clips/DC vs KXIP IPL 2nd match highlights HD 2020 ipl2020.mp4"

output_video_path = 'final.mp4'
fps = 30

# Process the video
process_video(video_path, output_video_path, model, condition_func, fps)



"""# Saving time stamps too"""

import cv2
import json
import torch

def process_video(video_path, output_video_path, model, condition_func, json_output_path, fps=30):
    vidcap = cv2.VideoCapture(video_path)
    success, frame = vidcap.read()

    if not success:
        print("Failed to open video")
        return

    # Video properties
    height, width, _ = frame.shape
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

    frame_count = 0
    highlights = []
    in_highlight_segment = False
    segment_start_time = None
    max_confidence = 0.0

    while success:
        # YOLO detection
        results = model(frame)
        boxes = results.xyxy[0].cpu().numpy()  # Convert bounding boxes to numpy array

        # Analyze if frame should be included
        if condition_func(frame, boxes):
            current_time = vidcap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0  # Current time in seconds

            if not in_highlight_segment:
                # Start of a new highlight segment
                in_highlight_segment = True
                segment_start_time = current_time
                max_confidence = max(box[4] for box in boxes)  # Highest confidence score in this segment

            # Update max confidence if this frame has higher confidence detections
            max_confidence = max(max_confidence, max(box[4] for box in boxes))

            # Draw bounding boxes and labels on the frame
            for box in boxes:
                x1, y1, x2, y2, conf, cls = int(box[0]), int(box[1]), int(box[2]), int(box[3]), box[4], int(box[5])
                label = model.names[int(cls)]
                confidence = box[4]

                # Draw the bounding box
                frame = cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

                # Prepare the label text
                label_text = f'{label} {confidence:.2f}'
                label_size, _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)
                label_ymin = max(y1, label_size[1] + 10)

                # Draw the label background
                frame = cv2.rectangle(frame, (x1, label_ymin - label_size[1] - 10),
                                      (x1 + label_size[0], label_ymin), (0, 255, 0), cv2.FILLED)

                # Put the label text above the bounding box
                frame = cv2.putText(frame, label_text, (x1, label_ymin - 7),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)

            # Write frame to output video
            out.write(frame)

        else:
            if in_highlight_segment:
                # End of the current highlight segment
                segment_end_time = vidcap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0  # Current time in seconds
                highlights.append({
                    'start_time': segment_start_time,
                    'end_time': segment_end_time,
                    'confidence': float(max_confidence)
                })
                in_highlight_segment = False
                max_confidence = 0.0

        # Read next frame
        success, frame = vidcap.read()â™ 
        frame_count += 1

    # Check if the last segment was not closed
    if in_highlight_segment:
        segment_end_time = vidcap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0
        highlights.append({
            'start_time': segment_start_time,
            'end_time': segment_end_time,
            'confidence': float(max_confidence)  # Ensure confidence is a float
        })

    vidcap.release()
    out.release()

    # Save highlights to JSON file
    with open(json_output_path, 'w') as f:
        json.dump(highlights, f, indent=4)

    print(f"Processed {frame_count} frames.")
    print(f"Highlights saved to {json_output_path}")

def condition_func(frame, boxes):
    return any(box[4] > 0.8 and box[4] < 0.95 for box in boxes)

# Parameters
video_path = "/kaggle/input/7-min-long-video-test/y2mate.com - 6  6  6  Shahid Afridi vs Chris Woakes  Pakistan vs England  2nd T20I 2015  PCB  MA2A_1080pFH.mp4"
output_video_path = 'final_7_min.mp4'
json_output_path = 'video_highlights-7-min.json'
fps = 30

# Load YOLO model
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)

# Process the video
process_video(video_path, output_video_path, model, condition_func, json_output_path, fps)

